\section{Experimental Results}
\label{sec:pmc-results}

This section presents comprehensive results for predicting memory consumption in seismic workloads based on their \ac{3D} input shapes.
We organize the findings into five main categories: (i) memory and execution-time profiling, (ii) model performance overview, (iii) feature selection experiments, (iv) data reduction studies, and (v) cross-operator comparisons.
Each category reveals different facets of how shape parameters affect resource requirements, model behavior, and prediction robustness.
All code, scripts, and output files that underlie these analyses reside in the \texttt{experiment} directory of the project repository~\cite{delucca2025experiment2results}.

\subsection{Experiment Outputs Overview}
\label{subsec:pmc-results-experiment-outputs-overview}

This subsection briefly outlines the three seismic operators examined (Envelope, \ac{GST3D}, and the 3D Gaussian Filter), underscores the main experimental objectives, and describes the key \ac{CSV} files generated during the final stages of the pipeline.
These \ac{CSV} artifacts are the foundation for the analyses presented in the following subsections.

\vspace{1em}
\noindent
\textbf{Operators and Experimental Goals.}
We investigated three commonly used seismic processing operators:
\begin{itemize}
    \item \emph{Envelope}: Computes instantaneous amplitude along seismic traces.
    \item \emph{\ac{GST3D}}: Highlights discontinuities or faults using structural tensors.
    \item \emph{3D Gaussian Filter}: Applies a smoothing operation across volumes to reduce high-frequency noise.
\end{itemize}
All three involve memory-intensive operations on potentially large \ac{3D} volumes.
Our primary goal was to build regression models that predict each operator’s peak memory usage as a function of shape parameters (\textit{inlines}, \textit{xlines}, and \textit{samples}), enabling more accurate \ac{HPC} job submissions.

\vspace{1em}
\noindent
\textbf{Data Artifacts.}
During the final stage of the pipeline, five main \ac{CSV} outputs were generated:
\begin{enumerate}
    \item \emph{profile\_history.csv}: Stores time-series \ac{RSS} measurements and timestamps for each operator run. This file is useful for detecting moment-to-moment memory fluctuations.
    \item \emph{profile\_summary.csv}: Summarizes peak memory usage, execution time, and statistical descriptors (means, standard deviations, minima, and maxima) per volume configuration.
    \item \emph{model\_metrics.csv}: Captures regression model performances (\ac{RMSE}, \ac{MAE}, $R^2$, and accuracy metrics), along with arrays of predictions and residuals.
    \item \emph{feature\_selection.csv}: Documents experiments that limit the predictor set to specific features or transformations to assess their importance in predicting memory usage.
    \item \emph{data\_reduction.csv}: Compares how models behave when trained on progressively smaller subsets of the full dataset, providing insight into data requirements for stable predictions.
\end{enumerate}

\vspace{1em}
\noindent
\textbf{Shape Configurations.}
The synthetic seismic volumes encompassed a broad range of \ac{3D} shapes, from $100 \times 100 \times 100$ up to sizes near the system’s memory limits.
By spanning both small and large volumes, we could characterize memory usage trends across diverse problem scales.
Each operator was run on each shape variant, yielding a comprehensive grid of memory and runtime measurements.

\vspace{1em}
\noindent
\textbf{Chapter Roadmap.}
Section~\ref{sec:pmc-results-memory-and-execution-time-profiling} explores the memory-usage trends and execution-time patterns for each operator.
Subsequent sections delve into model performance, assessing how various feature sets and data volumes influence predictive accuracy.
We conclude by comparing Envelope, \ac{GST3D}, and Gaussian Filter side by side, identifying operator-specific nuances that can inform more reliable \ac{HPC} scheduling.


\section{Memory and Execution-Time Profiling}
\label{sec:pmc-results-memory-and-execution-time-profiling}

In this section, we focus on raw memory and execution-time measurements before discussing any modeling approach.
By examining usage patterns, run durations, and dimension-specific behaviors, we contextualize the challenges of predicting peak memory consumption in an \ac{HPC} setting.

\subsection{Linear Trends and Variability}
\label{subsec:linear-trends-and-variability}

Figure~\ref{fig:peak_memory_facet} shows how average peak memory usage tends to scale linearly with the overall volume (\(\text{inlines} \times \text{xlines} \times \text{samples}\)). The Envelope, \ac{GST3D}, and Gaussian Filter operators each display a relatively direct proportionality between volume and memory demand.
Interestingly, larger volumes produce consistently higher yet less variable memory consumption, whereas smaller volumes display higher \ac{CV} (coefficient of variation).

A possible explanation for this disparity is that smaller volumes involve shorter-run processes in which overheads (e.g., Python interpreter initialization, I/O buffering) can appear more prominently, thus creating variability.
As volumes grow, these overheads become negligible compared to the large data arrays and associated computations.
Consequently, memory usage “smooths out” and increasingly reflects the operator’s intrinsic workload characteristics.

\begin{figure*}[htbp]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/images/05/peak_memory_by_volume_envelope}
        \caption{Envelope: Smaller volumes exhibit higher variability, while larger volumes show a consistent linear growth trend.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/images/05/peak_memory_by_volume_gst3d}
        \caption{\ac{GST3D}: This operator has a steeper slope compared to Envelope, reflecting more elaborate data structures.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/images/05/peak_memory_by_volume_gaussian-filter}
        \caption{Gaussian Filter: Memory usage also scales linearly but with a gentler slope than \ac{GST3D}.}
    \end{subfigure}
    \caption{Peak memory usage by volume for Envelope, \ac{GST3D}, and Gaussian Filter. Each operator shows an approximate linear growth rate as volume increases, but the slope and variability differ.}
    \label{fig:peak_memory_facet}
\end{figure*}

In parallel with rising memory usage, Figure~\ref{fig:execution_time_by_volume_facet} (detailed below) confirms that execution times also follow a predominantly linear trajectory with volume.
These combined observations suggest that volume—and more broadly, input shape—acts as a dominant factor in resource demands for seismic workloads.

\begin{figure*}[htbp]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \includegraphics[width=\textwidth]{assets/images/05/execution_time_by_volume_envelope}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/images/05/execution_time_by_volume_gst3d}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/images/05/execution_time_by_volume_gaussian-filter}
    \end{subfigure}
    \caption{Execution time by volume for Envelope, \ac{GST3D}, and Gaussian Filter. All three operators show an increasing trend, reinforcing the impact of volume on processing duration.}
    \label{fig:execution_time_by_volume_facet}
\end{figure*}

\subsection{Execution Time Distributions and Scaling Factor}
\label{subsec:execution-time-distributions-and-scaling}

To complement the volume-based analysis, Figures~\ref{fig:ex_peak_mu_facet}(a)--(b) compare execution time and peak memory usage across all operators.
Both metrics escalate in near-lockstep with input size.
Figure~\ref{fig:ex_peak_mu_facet}(c) then quantifies memory scaling slopes via linear-fits for each operator’s average peak \ac{RAM} usage. \ac{GST3D} stands out for having the highest slope, suggesting it holds more intermediate data structures during discontinuity detection.
Envelope lies in the middle, while Gaussian Filter’s slope is relatively smaller, implying that it processes data blocks more incrementally.

\begin{figure*}[htbp]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/images/05/cross_execution_time_by_volume}
        \caption{Execution time vs. volume for all three operators.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/images/05/cross_peak_memory_by_volume}
        \caption{Peak memory usage vs. volume for all three operators.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/images/05/cross_operator_memory_scaling_factor}
        \caption{Linear-fit slope values (GB/volume). Higher values indicate more pronounced memory growth.}
    \end{subfigure}
    \caption{Combined view of execution time and peak memory usage by volume, and a bar chart revealing the linear-fit slopes for Envelope, \ac{GST3D}, and Gaussian Filter. \ac{GST3D} uses memory most aggressively, while Gaussian Filter is comparatively more memory efficient.}
    \label{fig:ex_peak_mu_facet}
\end{figure*}

In addition, the run durations exhibit right-skewed behavior (Figure~\ref{fig:execution_time_distribution_facet}), consistent with gamma- or lognormal-like distributions.
While most executions concentrate in the lower range, occasional runs can be significantly longer.
Such tails are not unexpected in \ac{HPC} environments, where system-level noise or particular dataset structures can cause prolonged I/O or memory allocation overhead.

\begin{figure*}[htbp]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/images/05/execution_time_distribution_envelope}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/images/05/execution_time_distribution_gst3d}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/images/05/execution_time_distribution_gaussian-filter}
    \end{subfigure}
    \caption{Execution time distributions for Envelope, \ac{GST3D}, and the Gaussian Filter. Each distribution displays a heavy right tail, indicating that while most runs finish quickly, certain configurations or system conditions can cause notable slowdowns.}
    \label{fig:execution_time_distribution_facet}
\end{figure*}

\subsection{Dimension-Specific and Time-Progression Analysis}
\label{subsec:dimension-specific-and-time-progression-analysis}

To better understand how different axes of the seismic volume factor into resource usage, we break down memory usage by \emph{inlines}, \emph{xlines}, and \emph{samples} for the Envelope operator (Figure~\ref{fig:memory_usage_by_configuration_envelope}).
We observe that each dimension exerts a similar influence on memory consumption; no single dimension overwhelmingly dominates the Envelope’s consumption pattern.
This outcome aligns with the notion that Envelope is an element-wise amplitude calculation, depending uniformly on the size of the entire input array.

\begin{figure*}[htbp]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/images/05/memory_usage_by_configuration_envelope}
        \caption{Memory usage binned by individual shape parameters. Each axis increases overall memory similarly.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/images/05/execution_time_vs_memory_envelope}
        \caption{Execution time vs. memory usage for Envelope, indicating a mild correlation (longer runs often consume more \ac{RAM}).}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/images/05/memory_usage_inlines_xlines_samples_heatmap_envelope}
        \caption{Heatmap combining inlines, xlines, and samples into a 2D view; warmer colors indicate higher memory usage.}
    \end{subfigure}
    \caption{Memory usage and runtime analysis for the Envelope operator. All three dimensions appear to scale memory demand in a similar manner, and bigger shapes generally require longer processing times.}
    \label{fig:memory_usage_by_configuration_envelope}
\end{figure*}

Figure~\ref{fig:inline_xline_memory_usage_progression_envelope} further highlights how the Envelope operator’s memory usage accumulates over time.
The growth pattern is relatively uniform, aligning with the straightforward element-wise nature of the algorithm.
More complex operators like \ac{GST3D} may reveal steeper or stage-based ramps, particularly if certain algorithmic phases allocate additional buffers or caching mechanisms at specific intervals.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{assets/images/05/inline_xline_memory_usage_progression_envelope}
    \caption{Memory usage progression over time (Envelope operator) for different inlines and xlines. The consumption profile ramps up gradually rather than spiking in distinct phases.}
    \label{fig:inline_xline_memory_usage_progression_envelope}
\end{figure}

\subsection{Memory Safety Margins}
\label{subsec:memory-safety-margins}

In real-world \ac{HPC} contexts, underestimating memory can cause abrupt job failures, while overestimation wastes resources.
Figure~\ref{fig:memory_safety_margin} showcases how much peak usage can deviate from average usage by comparing mean and 95th-percentile values.
Envelope and \ac{GST3D} exhibit particularly large deviations for certain volume ranges, hinting that HPC practitioners may need to add a safety buffer beyond the mean consumption to accommodate transient memory spikes.

\begin{figure*}[htbp]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/images/05/memory_safety_margin_envelope}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/images/05/memory_safety_margin_gst3d}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/images/05/memory_safety_margin_gaussian-filter}
    \end{subfigure}
    \caption{Memory safety margins for Envelope, \ac{GST3D}, and the Gaussian Filter. The 95th-percentile often surpasses mean usage by a noticeable margin, revealing the unpredictability of peak memory events.}
    \label{fig:memory_safety_margin}
\end{figure*}

These outliers can arise from transient allocation bursts, system-level scheduling, or overhead fluctuations.
Even with container-based isolation, kernel-level memory management can introduce sporadic spikes.
Recognizing these high-percentile events is critical for designing robust pre-runtime memory estimators.
If left unaccounted for, such transient peaks could lead to underestimation, especially for borderline volumes.

\subsection{Dimension Correlations}
\label{subsec:dimension-correlations}

We can further investigate how dimension-specific growth patterns contribute to overall memory usage by plotting pairwise relationships between memory usage and the three shape parameters.
Figure~\ref{fig:memory_vs_dimensions_pairplot_gst3d} shows that \ac{GST3D}, in particular, has strong positive correlations between each dimension and peak memory.
Meanwhile, dimension--dimension scatter plots sometimes show inverse correlations due to the systematic way shapes were varied (when one axis is large, another might be slightly smaller).
Nevertheless, once all three dimensions expand simultaneously, the net memory usage grows sharply.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{assets/images/05/memory_vs_dimensions_pairplot_gst3d}
    \caption{Pairplot for \ac{GST3D} memory usage against inlines, xlines, and samples. Positive slopes in memory-related plots confirm that larger shape parameters escalate the overall memory footprint.}
    \label{fig:memory_vs_dimensions_pairplot_gst3d}
\end{figure}

These pairwise correlations, combined with the linear volume trend, underscore that each dimension matters and that polynomial or interaction-based features can help capture how memory usage evolves in less trivial cases (e.g., operators whose complexity grows nonlinearly along certain axes).

\subsection{Summary of Observed Resource Usage}
\label{subsec:resource-usage-summary}

Table~\ref{tab:operator_summary_aggregates} provides a high-level summary of all tested volume ranges, memory usage spans, and measured execution times for each operator.
It highlights how, for similarly sized volumes, \ac{GST3D} consistently demands the greatest \ac{RAM}, with Envelope requiring intermediate amounts, and Gaussian Filter at the lower end (though still significant).
Processing times echo these patterns, with more complex or data-hungry operations tending to take longer.

\begin{table}[htbp]
    \centering
    \begin{tabular}{lcccc}
        \hline
        \textbf{Operator} & \textbf{Volume Range} & \textbf{Peak Mem. Usage (GB)} & \textbf{Exec. Time (s)} \\ \hline
        Envelope &
        $10^6 \!\to\! 6.4\times10^7$ &
        0.10 -- 1.76 &
        0.0106 -- 0.5025 \\
        \ac{GST3D} &
        $10^6 \!\to\! 2.7\times10^7$ &
        0.31 -- 6.12 &
        0.2475 -- 7.75 \\
        Gaussian Filter &
        $10^6 \!\to\! 6.4\times10^7$ &
        0.10 -- 0.57 &
        0.0232 -- 1.22 \\
        \hline
    \end{tabular}
    \caption{Resource usage summary for Envelope, \ac{GST3D}, and Gaussian Filter.
    Volumes are specified in number of elements (e.g., $100 \times 100 \times 100 = 10^6$).
    Memory usage is in GB and time is in seconds.
    Each range denotes the min and max observed across tested volumes.}
    \label{tab:operator_summary_aggregates}
\end{table}

In general, both memory and runtime grow at a near-linear rate, reinforcing the fundamental role of volume in resource demands.
These findings suggest that shape-driven models are likely to be quite effective, especially if they incorporate slight nonlinearities or additional features to handle outliers.
The next sections delve into how these raw measurements translate into model predictions, discussing which methods and features best capture the trends and variability described above.


\section{Model Performance Overview}
\label{sec:pmc-results-model-performance-overview}

%- chart assets/images/05/actual_vs_predicted_by_model.pdf shows the results per model and operator. It compares the actual and predicted values. It is clear that most of the models got a pretty good result, but a few of those we can highlight. Namely linear regression,xgboost,and elastic net went pretty well for all models
%- chart assets/images/05/residual_vs_predicted.pdf shows clearly that most models got pretty low residuals. Really close to 0. A few models for a few operators went pretty bad, specially for the gst3d operator
%- chart assets/images/05/cross_model_r2_bar.pdf shows all models r2 scores per operator. We can see that the neural network was the only that perform poorly on gst3d, and we can corroborate the results from the previous chart, that the linear regression, xgboost, and elastic net were the best models
%- charts assets/images/05/score_by_model_envelope.pdf, assets/images/05/score_by_model_gst3d.pdf, assets/images/05/score_by_model_gaussian_filter.pdf show the score for each model per operator. We can see that the best model varies depending on the operator, but we have many models that are really close. For envelope the best model was gradient boost, with a score of 2,579 while for gaussian filter many models performed well, being linear regression the best with a score of 2,904. For gst3d the top score was 2,970 for the decision tree model
%- chart assets/images/05/best_model_per_operator.pdf shows that in a single chart (what was mentioned above)


\section{Feature Selection Experiments}
\label{sec:pmc-results-feature-selection-experiments}


\section{Data Reduction Studies}
\label{sec:pmc-results-data-reduction-studies}


\section{Summary of Findings}
\label{sec:pmc-results-summary-of-findings}
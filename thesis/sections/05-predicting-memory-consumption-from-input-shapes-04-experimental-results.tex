\section{Experimental Results}
\label{sec:pmc-results}

\subsection{Experiment Outputs Overview}
\label{subsec:pmc-results-experiment-outputs-overview}

This section introduces the operators under study, highlights the main experimental goals, and outlines key data artifacts.
The final segments of the pipeline generated five distinct \ac{CSV} files that consolidate memory usage measurements, model evaluations, feature selection outcomes, and data-reduction experiments.
These artifacts form the basis for subsequent analyses in the following sections.

\vspace{1em}
\noindent
\textbf{Operators and Experimental Goals.}
Three operators were evaluated: Envelope, \ac{GST3D}, and the 3D Gaussian Filter.
All three process seismic data volumes and exhibit distinct computational traits.
The primary objective involved modeling their peak \ac{RAM} consumption based on shape parameters (inlines, xlines, and samples), thereby enabling more informed \ac{HPC} resource allocations.

\vspace{1em}
\noindent
\textbf{Data Artifacts.}
The experiment produced five core \ac{CSV} files:
\begin{enumerate}
    \item \emph{profile\_history.csv}:
    Time-series snapshots of \ac{RSS} and corresponding timestamps.
    Each row represents one data capture during an operator’s execution.
    \item \emph{profile\_summary.csv}:
    Aggregated statistics about peak memory usage and execution time per input volume.
    This file stores the mean, standard deviation, minimum, and maximum peak memory usage, along with average runtimes and related descriptors.
    \item \emph{model\_metrics.csv}:
    Performance metrics (\ac{RMSE}, \ac{MAE}, $R^2$, accuracy, and an overall score) for each trained regression model.
    It also includes arrays of residuals, predicted values, and ground truths for deeper error analysis.
    \item \emph{feature\_selection.csv}:
    Results of experiments that use different subsets of features to predict memory usage.
    Each row documents the selected features, model configuration, and associated performance (\ac{RMSE}, \ac{MAE}, $R^2$, accuracy, and score).
    \item \emph{data\_reduction.csv}:
    Outputs from varying the number of training samples.
    The file compares performance metrics across multiple training-set sizes to study model robustness under data scarcity.
\end{enumerate}

\vspace{1em}
\noindent
\textbf{Shape Configurations.}
The synthetic datasets spanned multiple \ac{3D} dimensions.
Volumes ranged from smaller shapes (e.g., $100$ $\times$ $100$ $\times$ $100$) to significantly larger ones close to memory limits of the test system.
Each operator was executed for these volumes to capture comprehensive memory-profiling data.
This systematic approach reveals how memory usage and execution time evolve as the number of traces or samples grows.

\vspace{1em}
\noindent
\textbf{Chapter Roadmap.}
Section~\ref{sec:pmc-results-memory-and-execution-time-profiling} first presents how memory usage behaved during operator runs and highlights the aggregated profiling statistics.
Subsequent sections detail model performance across regression approaches, emphasizing how different features and training subsets affect accuracy.
The final portion compares Envelope, \ac{GST3D}, and Gaussian Filter side by side to underscore differences and provide practical insights for \ac{HPC} scheduling.


%5.2 Memory and Execution-Time Profiling
%
%This section uses:
%•	\texttt{profile_history.csv}:
%•	Detailed, time-series-like snapshots of memory usage (\texttt{captured_memory_usage}) and relative timestamps for each session.
%•	Allows visualization of how memory ramps up or remains stable across the operator’s execution.
%•	\texttt{profile_summary.csv}:
%•	Aggregated statistics (average peak memory usage, min/max, standard deviation, and execution times).
%•	Useful for comparing memory usage across different volumes or operators.
%
%5.2.1 Time-Series Patterns (Using \texttt{profile_history.csv})
%•	Goal: Show how memory usage progresses over time for a given shape configuration.
%•	Possible Figures:
%•	Plots of \texttt{captured_memory_usage} vs \texttt{relative_time} for select volumes/operators.
%•	Overlay or small multiples to illustrate typical vs extreme cases.
%•	Discussion Points:
%•	Identify whether memory usage spikes early or gradually increases.
%•	Note if operators differ in how fast memory peaks or whether they hold a stable plateau.
%
%5.2.2 Aggregated Statistics (Using \texttt{profile_summary.csv})
%•	Goal: Compare memory usage (peak, average, standard deviation) and execution times across volumes.
%•	Possible Figures:
%•	Scatter plots of \texttt{peak_memory_usage_avg} vs volume.
%•	Bar or box plots comparing \texttt{execution_time_avg} for various input volumes.
%•	Possibly a correlation chart between memory usage and execution time.
%•	Discussion Points:
%•	How memory usage scales with volume for each operator.
%•	Relationship between memory usage and runtime (e.g., does higher memory usage always mean longer runtimes or not?).
%•	Identify the presence (or absence) of outliers in \texttt{peak_memory_usage_max} vs \texttt{peak_memory_usage_min}.
%
%⸻
%
%5.3 Model Performance Overview
%
%This section focuses on:
%•	\texttt{model_metrics.csv}:
%•	Overall performance metrics of each model (Linear Regression, \ac{XGBoost}, etc.).
%•	Columns include \texttt{rmse}, \texttt{mae}, \texttt{r2}, \texttt{accuracy}, and a combined \texttt{score}.
%•	Residual arrays (\texttt{residuals}), predicted values (\texttt{y_pred}), and ground truths (\texttt{y_test}) are also stored.
%
%5.3.1 Global Comparison Across Models
%•	Goal: Show how each model fares when predicting memory usage across the full dataset (per operator).
%•	Possible Figures/Tables:
%•	A table summarizing \texttt{rmse}, \texttt{mae}, \texttt{r2} for each model.
%•	Bar chart or grouped bar for direct comparison of \texttt{score} or \texttt{accuracy}.
%•	Discussion Points:
%•	Which model emerges as the top performer overall.
%•	Whether simpler models (Linear, Polynomial) capture memory scaling or if tree-based methods (\ac{XGBoost}, Random Forest) do better.
%
%5.3.2 Error and Residual Analysis
%•	Goal: Investigate how well the best models predict memory usage for different volumes or shapes.
%•	Possible Figures:
%•	Residual vs predicted-value plots (from the \texttt{residuals} and \texttt{y_pred} columns).
%•	Distribution or histogram of residuals, or QQ-plots to check normality.
%•	Discussion Points:
%•	Whether residuals cluster around zero for small volumes but diverge for large volumes.
%•	Potential systematic underestimation or overestimation in certain volume ranges.
%
%⸻
%
%5.4 Feature Selection Experiments
%
%This section references:
%•	\texttt{feature_selection.csv}:
%•	Stores metrics after training the model with different subsets of features.
%•	Includes \texttt{num_features}, a list of \texttt{selected_features}, and the resulting \texttt{rmse}, \texttt{mae}, \texttt{r2}, etc.
%
%5.4.1 Impact of Feature Subsets on Performance
%•	Goal: Understand which combination of features yields minimal error and whether diminishing returns appear with too many features.
%•	Possible Figures:
%•	Line chart showing \texttt{rmse} (or \texttt{r2}) vs \texttt{num_features}.
%•	Bar chart of the final feature importances (if your best model is tree-based, you can show its built-in feature importance).
%•	Discussion Points:
%•	Identify key features (e.g., \texttt{volume}, \texttt{log_volume}, \texttt{surface_area}).
%•	Whether advanced polynomial or ratio-based features significantly improve accuracy, or if simple dimension-based ones are sufficient.
%
%⸻
%
%5.5 Data Reduction Studies
%
%This section uses:
%•	\texttt{data_reduction.csv}:
%•	Shows how performance changes when training on fewer samples (\texttt{num_samples}).
%•	Metrics (\texttt{rmse}, \texttt{mae}, \texttt{r2}, \texttt{accuracy}) indicate how robust the model is with less data.
%
%5.5.1 Sensitivity to Training-Set Size
%•	Goal: Show the extent to which the model can still predict well if fewer shape configurations or runs are available.
%•	Possible Figures:
%•	Line plots of \texttt{rmse} vs \texttt{num_samples}, or a multi-metric plot with \texttt{rmse}, \texttt{mae}, and \texttt{r2} all displayed.
%•	Discussion Points:
%•	Minimal training set requirements for a stable predictor.
%•	Significant performance drops below a certain threshold of \texttt{num_samples}.
%
%⸻
%
%5.6 Cross-Operator Comparisons
%•	Purpose: Compare Envelope, \ac{GST3D}, and Gaussian Filter side by side.
%•	Possible Content:
%•	Summaries of the best model’s performance for each operator (from \texttt{model_metrics.csv}).
%•	Memory usage vs time scaling differences (from \texttt{profile_summary.csv}).
%•	Discussion Points:
%•	Whether one operator exhibits a more predictable memory footprint (e.g., Envelope might be more linear than \ac{GST3D}).
%•	Whether operators require different feature sets or if the same core features suffice to get good results.
%
%⸻
%
%5.7 Summary of Findings
%•	Purpose: Recap the main takeaways of the chapter.
%•	Possible Talking Points:
%•	Which model consistently performed best.
%•	How memory usage scales with volume and which features truly matter.
%•	The threshold of training data needed to maintain strong performance.
%•	Operator-specific quirks and real-world implications for \ac{HPC} resource requests.
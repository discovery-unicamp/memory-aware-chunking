%\section{Experimental Results}
%\label{sec:pmc-results}
%
%Overall Prediction Accuracy: The results show that memory consumption can be predicted with high accuracy from input shapes. Most models captured the general trend that larger inputs require more memory, and several models achieved near-perfect fit on the test data. For instance, the best-performing model achieved an R² above 0.95 on the synthetic test cases, indicating that almost all variance in memory usage was explained by the input shape features. In concrete terms, for a typical large input in our test set (e.g., a 200×200×200 volume), the predicted peak memory differed from the actual by only a few percent. This level of accuracy is far better than the manual trial-and-error estimates users typically resort to, and is on par or better than similar memory prediction efforts reported in literature (which often report R² in the 0.7–0.9 range for more generalized workloads) (https://pmc.ncbi.nlm.nih.gov/articles/PMC9906793/#:~:text=Management%20%28Slurm%29,consumption%20of%20the%20HPC%20resources). Importantly, even when not exact, the predictions tended to err on the safe side, slightly overestimating rather than underestimating the memory in most cases – a desirable outcome for practical use.
%• Comparison of Models: We compared the performance of all models (Linear, Polynomial, Tree-based, Ensemble, Neural Network, Gaussian Process, Bayesian Ridge) using both the standard metrics and the custom underestimation-penalizing score. Key findings include:
%• Linear vs. Polynomial: The simple linear regression, using total number of elements as a feature, already provided a reasonable first approximation. It captured the general linear growth of memory with input size but under-predicted memory for very large inputs by a noticeable margin (e.g., for the largest synthetic case, linear prediction was ~10% lower than actual). This underestimation suggests a slight nonlinear component (perhaps a fixed overhead or higher-order term) that a pure linear model couldn’t capture. Polynomial regression (we experimented with quadratic and cubic terms) significantly improved the fit. A quadratic model, for example, reduced the error for large inputs and aligned closely with actual usage across the range, implying that the memory-size relationship has a curve that a second-order term can model. However, beyond a certain degree, polynomial models started to overfit the training data (oscillating between points), so we found a low-degree polynomial to be optimal.
%• Tree-Based Models: The single decision tree model did learn the relationship fairly well, creating splits mostly on the total number of elements feature. It had low error on training data but was slightly less accurate on test data due to some overfitting (the piecewise constant nature of a tree meant minor overshoots or undershoots in certain ranges). The power of tree models became evident with Random Forest and Gradient Boosting. The random forest smoothed out the decision tree’s piecewise predictions by averaging many trees, resulting in a robust model that was accurate to within a few percent for all test points. It rarely under-shot the actual memory – in fact, the ensemble’s tendency to average out noise meant it usually predicted a touch above the actual (providing a buffer). Gradient Boosting and XGBoost delivered the most accurate predictions among all models. After tuning (e.g., limiting tree depth to prevent overfitting small quirks), XGBoost achieved an almost one-to-one fit: when plotting predicted vs. actual memory, the points lay very close to the diagonal line. These ensemble models earned top scores in our custom metric as well, since they virtually never under-predicted the peak memory. This aligns with other research that found boosted trees to excel in resource usage prediction tasks (https://pmc.ncbi.nlm.nih.gov/articles/PMC9906793/#:~:text=from%20January%202019%20,consumption%20of%20the%20HPC%20resources).
%• Neural Network: Our neural network (a feed-forward model with a couple of hidden layers) also managed to learn the relationship after training. Its performance was comparable to the ensemble models in terms of mean error; however, one key difference was that the neural net exhibited slightly higher variance in error for different test cases. In other words, most predictions were spot-on, but a few were off by a larger margin (both over and under). We suspect this is due to the neural network not having as many training samples as it would ideally want – with more data or further hyperparameter tuning, it might match the consistency of the random forest. Nonetheless, it did not show any systematic bias and generally predicted safe values. The advantage of the neural network is that it could potentially incorporate additional inputs easily (if we had more features like algorithm settings), but in our shape-only scenario, it didn’t dramatically outperform the structured models.
%• Gaussian Process: The GP regressor yielded excellent results on the interpolation range of our synthetic data. Given its nature, it essentially provided an almost exact fit through the training points (since we assumed a relatively smooth kernel). On test points within the range of training shapes, the GP’s predictions were extremely close to actual usage, and it provided tight confidence intervals, signaling high certainty. One challenge with the GP emerged when predicting for the extreme ends (for example, the absolute largest shape which might be slightly beyond the range of training data). In those cases, the GP’s confidence interval widened, and we observed a slight underestimation for one extrapolation case. This is expected behavior – the model was less certain and effectively regressed toward the mean. In practice, this is useful: the GP can warn us when a prediction is an extrapolation by showing high uncertainty. For our purposes, the GP was among the top performers for interpolation, and careful handling of its predictions at extremes could make it a reliable choice.
%• Bayesian Ridge Regression: The Bayesian Ridge model performed similarly to the ordinary linear regression, as anticipated. The added regularization helped a bit in not overshooting any coefficients, but since the true relationship isn’t strictly linear, it also under-predicted for the largest inputs. It did, however, give a probabilistic prediction (with a variance) that indicated low confidence in those large-input predictions – which is an insight that plain linear regression wouldn’t provide. This mirrors the finding that a simple linear model is slightly inadequate, but the fact that it wasn’t wildly wrong means the assumption of memory roughly scaling with input size is fundamentally sound.
%• Quantitative Performance Summary: (We will present a table or chart here comparing each model on metrics like MSE, R², and the custom score, as well as noting under/over prediction tendencies.) In summary, the ensemble tree models (Random Forest, XGBoost) and the Gaussian Process were the stand-out performers with highest accuracy and safest predictions. For example, XGBoost achieved an R² ≈ 0.98 on the test set and had a custom score indicating virtually zero dangerous underestimations. The neural network was a close second-tier, with R² in the ~0.95 range. Polynomial regression (quadratic) also did quite well (R² ~0.92), proving that a fairly simple curve-fit already captures most of the memory behavior. Linear regression and Bayesian Ridge trailed with R² in the 0.85–0.90 range, mainly due to systematic underestimation on large cases. The single decision tree had high training R² but test R² around 0.88, reflecting some overfit piecewise behavior. These results confirm that the relationship between input shape and memory is learnable and mostly smooth, as simpler models weren’t far off, but the best accuracy is achieved with models that can handle a slight nonlinearity and interaction in the data.
%• Actual vs. Predicted Plots: We include plots of actual vs. predicted memory for selected models to visually illustrate performance. In the case of XGBoost (our best model), the scatter plot of predicted vs. actual memory (in GB) for all test points lies almost on the $y=x$ line, with perhaps a slight buffer above the line for the largest points (indicating slight overestimation). This visual confirms the high accuracy. For linear regression, the plot shows a trend line that falls under the $y=x$ line at the high end, consistent with under-predicting large values. We also plot the error (residual) vs. input size, which shows linear regression’s residuals growing with input size, whereas the residuals for XGBoost stay centered around zero across the whole range. These figures reinforce why the more sophisticated models were needed for top precision. (If space allows, we might also show the GP’s prediction with confidence bands as a function of input size – demonstrating how uncertainty grows outside the training range).
%• Performance on Real F3 Data: A crucial test was how well the models trained on synthetic data predict the memory usage for the real F3 seismic dataset. We found that the best models generalized well. For instance, the random forest and XGBoost predictions for the full F3 volume’s memory were within ~5% of the actual peak usage observed when we ran the algorithm on that data. This is a strong result, indicating that our synthetic training (which included shapes similar in scale to F3) was representative. The models essentially recognized that F3’s shape (255×901×601) was at the high end of the range and extrapolated the trend accurately. The neural network and GP also predicted close to the actual F3 memory, each slightly overestimating by a small margin (which in a real scenario is acceptable, as it would mean we’d allocate a bit more memory than needed, avoiding any risk). Simpler models like linear regression under-shot the F3 usage by more than 10% (which, if followed blindly, could have led to under-allocation). This validates our approach: by using more expressive models, we can trust the predictions even for a real-case input that was not explicitly in training. It also highlights the benefit of including high-end synthetic points in training so that models don’t need to extrapolate far beyond seen data.
%• Custom Metric & Safe Predictions: Using our custom “no-underestimation” score, we determined that several models (especially Random Forest, XGBoost, and GP) achieved perfect or near-perfect safety scores – meaning they never predicted less than the actual usage by more than a negligible amount on any test case. Models that had any significant misses (like linear regression’s 10% underestimation on the largest case) were ranked lower despite decent R². This exercise underscores an important point: what constitutes the “best” model depends on the practical criterion. If avoiding job crashes is paramount, one might choose a model that perhaps overestimates by a margin on every single point rather than one that is 99% accurate on average but occasionally 10% low. Fortunately, in our situation, we found models that were both highly accurate and safe. In deployments, one could even take an accurate model and add a small safety buffer to its predictions (e.g., predict memory = model output * 1.05 to cover any unknowns). In our results, we note that approach as a possible refinement, though for the top models it wasn’t really necessary given their performance.
%• Insights Gained: Through analysis of the learned models, we can glean insights about how the algorithm uses memory. The most dominant feature was unsurprisingly the total number of data points (or a close proxy like one dimension times another for 3D). This confirms the intuition that more data yields higher memory usage primarily linearly. The need for a quadratic term in polynomial regression or depth in trees suggests there is a fixed overhead or a second-order effect – possibly the algorithm allocates an extra array that scales with one of the dimensions (for example, maybe it allocates a temporary 2D slice array of size width×height, causing an additional term proportional to one plane of the volume). The models picked up on that. In fact, by inspecting the decision splits of the tree or the coefficients of a quadratic fit, we could guess at the formula: one such analysis hinted at $M \approx a \cdot (D \times H \times W) + b \cdot (H \times W) + c$ bytes (where the second term represents an extra array the size of one horizontal slice). This matches how one might manually derive the memory formula if one knew the algorithm’s internals. Thus, the ML approach not only provides predictions but can also validate our understanding of algorithmic memory usage.
%• Error Analysis: We also examined the few cases with larger prediction errors. One pattern observed was that when an input shape was very small, some models (especially polynomial and ensemble) slightly over-predicted memory. For example, for a tiny 10×10×10 test volume, the actual memory usage might include a base overhead (say 50 MB for loading libraries, etc.), which our models (trained mostly on larger sizes) approximated less precisely, leading to a prediction of perhaps 60 MB vs actual 55 MB. This is a minor issue, and in practice overallocation of a few MB is inconsequential, but it points to the models being primarily tuned for the scaling behavior rather than the absolute base offset. We could address this by including more small-size data points in training or by explicitly modeling base overhead as a constant term (which linear models do automatically as an intercept, but tree models might not capture as smoothly). We mention this for completeness, but emphasize it’s not a serious flaw.
%• Statistical Confidence: Finally, we assess the confidence in these results. Given the consistent performance across multiple validation splits and the agreement of different high-capacity models (trees, NN, GP) on the memory trend, we are confident that the dependency of memory on input shape has been learned robustly. The standard deviation of errors is low, and no model completely disagreed with theoretical expectations (no bizarre predictions). This consistency suggests the problem is well-conditioned for learning. The next section will discuss the implications of these findings and how such a predictive model can be applied in practical settings.
\section{Experimental Setup}
\label{sec:pmc-experimental-setup}

As mentioned in section~\ref{sec:pmc-materials-and-methods}, the experiment was executed as follows: we generated seismic data, ran a containerized workflow to process each dataset while recording memory usage, and finally aggregated the results into a single dataset for analysis and model fitting.
Below, we summarize each step in the pipeline and how it was automated.

\subsection{Execution Environment and Workflow Automation}
\label{subsec:pmc-execution-environment-and-workflow-automation}

All runs took place on a single \ac{HPC} node (Intel\textregistered\ Xeon\textregistered\ Silver 4310, 256 \ac{GB} \ac{RAM}, and two \ac{RTX}~A6000 \ac{GPU}s), located on the \ac{UNICAMP} Discovery Labs, using a Python-based container environment.
Although \ac{GPU} were available, our main memory experiments ran on \ac{CPU} to keep runs consistent across shapes.
Docker containers helped ensure reproducibility by freezing Python package versions and isolating each run.

We automated the entire workflow through a single shell script,
\texttt{scripts/experiment.sh}~\cite{delucca2025experiment2script}, which:
\begin{enumerate}
    \item \textbf{Created Docker volumes} for storing intermediate data and outputs.
    \item \textbf{Generated synthetic \ac{3D} seismic volumes} (Section~\ref{sec:pmc-materials-and-methods} describes the shapes and sizes chosen).
    \item \textbf{Executed the seismic operators} inside pinned containers (via \texttt{–cpuset-cpus=0}), one run per shape configuration.
    This \ac{CPU} pinning ensures consistent timing and minimal interference among runs.
    \item \textbf{Used \texttt{TraceQ}} to record memory consumption during execution, capturing \ac{RSS} over time.
    \item \textbf{Extracted and consolidated results}, including peak memory usage, shape metadata, and run logs, into \texttt{.csv} files.
\end{enumerate}

This script looped over all possible combinations of \texttt{(inlines, xlines, samples)}.
A final pass of the script parsed \texttt{TraceQ} output, joined shape information with peak memory usage, and produced a consolidated table for the modeling phase.

\subsection{Synthetic Data Generation and Real-Data Validation}
\label{subsec:pmc-synthetic-data-generation-and-real-data-validation}

\paragraph{Synthetic Volumes.}
We programmatically enumerated \ac{3D} array dimensions from smaller volumes (e.g., $100$ $\times$ $100$ $\times$ $100$) up to larger ones near the machine’s memory limit.
Each synthetic volume was written to a \ac{SEG-Y} file, ensuring consistent format for the seismic operators.
This step let us systematically cover a range of sizes that might never appear in real datasets but are critical for understanding worst-case memory needs.

\paragraph{Netherlands F3 Dataset~\cite{f3dataset}.}
For real-data validation, we worked with the standard Netherlands F3 seismic volume.
We used the full $651$ $\times$ $951$ $\times$ $462$ block and compared the predictions from our models against the actual memory usage.
This allowed us to validate our models against a real-world dataset and assess their generalization capabilities.

\subsection{Containerized Seismic Operators and Memory Profiling}
\label{subsec:containerized-seismic-operators-and-memory-profiling}

Each experiment (each shape) was run in a dedicated Docker container with pinned \ac{CPU} resources.
We employed three main seismic operators:
\emph{Envelope}, \emph{\ac{GST3D}}, and a \emph{3D Gaussian Filter}.
While their internal computations differ, from the outside we treated them as “black boxes” that receive seismic volumes and transform them in memory-intensive ways.

To log memory usage, the container startup script activated \texttt{TraceQ}, which hooks into the Python process and periodically snapshots \ac{RSS}.
When the operator finished, \texttt{TraceQ} saved a \texttt{.prof} file containing the entire memory timeline.
By isolating each run in its own container and controlling \ac{CPU} usage, we minimized cross-talk from other processes.

\subsection{Data Consolidation}
\label{subsec:data-consolidation}

After all runs completed, the script systematically merged:
\begin{itemize}
    \item \emph{Peak Memory Values:} The highest \ac{RSS} from each \texttt{.prof} file.
    \item \emph{Shape and Operator Metadata:} Inline, xline, and sample counts; which operator was used; and any other relevant metadata.
    \item \emph{Derived Features:} Total volume (\text{inlines} $\times$ \text{xlines} $\times$ \text{samples}), logarithms, and geometric attributes (surface area, diagonal length, etc.).
\end{itemize}

This merged dataset became the raw material for model fitting.
The final \texttt{.csv} files placed in \texttt{OUTPUT\_DIR} containing all runs, labeled with each operator, shape, and peak memory usage.

\subsection{Repository Structure and Notebooks}
\label{subsec:repository-structure-and-notebooks}

All code and instructions for reproducing the experiment are publicly available:
\begin{itemize}
    \item \textbf{\texttt{Github repository}}~\cite{delucca2025experiment2} which contains the full source code and instructions.
    \item \textbf{\texttt{scripts/experiment.sh}}~\cite{delucca2025experiment2script} automates data generation, operator execution, and memory profiling.
    \item \textbf{\texttt{notebooks/}}~\cite{delucca2025experiment2notebooks} folder contains detailed Jupyter notebooks covering data exploration, model training, hyperparameter tuning, and error analysis.
\end{itemize}

By keeping container definitions, experiment scripts, and analysis notebooks together, we ensure end-to-end reproducibility.
Anyone can tweak shape ranges, operator settings, or container parameters to replicate (or extend) these runs in different environments while preserving the same orchestration logic.
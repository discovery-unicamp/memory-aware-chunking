\chapter{Predicting Memory Consumption from Input Shapes}
\label{ch:predicting-memory-consumption-from-input-shapes}


\section{Introduction}
\label{sec:pmc-introduction}

\ac{HPC} environments require users to specify computing resources—especially memory—before a job begins execution.
Common \ac{HPC} job schedulers (e.g., Slurm~\cite{yoo2003slurm} or PBS~\cite{henderson1995pbs}) enforce fixed memory reservations at submission time, forcing the user to estimate a job’s memory footprint in advance.
This requirement presents a fundamental challenge: the user must estimate memory needs \emph{a priori}, often without the benefit of trial runs.
If the memory request is set too low, the job will fail with an out-of-memory error~\cite{bailey2005,hovestadt2003}, resulting in lost computation time and queued tasks.
Conversely, if the request is overly high, the job holds more memory than needed, idling precious resources and potentially lengthening queue wait times for other users.
Thus, accurate memory estimation is crucial for both reliability and efficiency in \ac{HPC} resource management.

In practice, memory reservation involves significant guesswork.
Many \ac{HPC} users resort to trial-and-error~\cite{ncsu_memory_usage}, incrementally adjusting memory limits across repeated job submissions until one run succeeds without errors.
This ad hoc strategy is inefficient and risky.
Each failed attempt due to insufficient memory wastes wall-clock time and computing cycles, and even successful runs with overly generous memory allocations underutilize hardware.
Over-provisioning memory as a safeguard is common, yet it leads to low system utilization as nodes sit with allocated but unused memory.
The trial-and-error paradigm not only diminishes overall throughput but also burdens users, who must divert focus to resource tuning rather than the scientific computation at hand.

There is a clear opportunity to improve this situation through predictive modeling techniques for memory allocation.
Rather than relying on human intuition or conservative heuristics, predictive models can forecast a job’s memory consumption based on easily observable features of the workload.
Such features might include input data characteristics (e.g., the size or dimensionality of input arrays), algorithmic parameters, or historical usage patterns from similar jobs.
By leveraging these predictors, a model can provide an informed estimate of the required memory before execution~\cite{tanash2021ampro}.
This data-driven approach reduces human guesswork and minimizes iterative adjustments.
More accurate pre-runtime memory predictions allow users to request just-enough memory, mitigating the risk of failures while curbing resource waste.

This chapter focuses on tensor-based workloads as a domain where memory usage correlates strongly with input shape.
Many scientific and data-intensive computations revolve around operations on large multi-dimensional arrays (tensors).
Notable examples include seismic wave simulations on \ac{3D} grids, high-resolution image analysis in computer vision, and large-scale numerical linear algebra in scientific computing.
In such cases, the storage of input data structures and intermediate results often dominates memory usage and scales directly with input dimensions.
For instance, doubling the resolution of an input image (in each dimension) roughly quadruples its memory footprint~\cite{stackoverflow_memory_inv}.
Similarly, in deep neural networks, increasing the input size or batch size can dramatically raise memory requirements due to larger activation maps and additional computation~\cite{dell_3dunet_memory}.
These patterns indicate that input shape is a primary determinant of memory consumption for tensor-centric workloads.
Exploiting this relationship, a predictive model can learn how memory scales with shape parameters and thereby forecast memory requirements for new input configurations.

Several lines of prior work have explored memory usage estimation and prediction in related contexts.
On the \ac{HPC} front, researchers have leveraged historical job data from systems like Slurm to predict resource usage.
For example, machine learning models trained on past job records (using features such as job metadata and input sizes) can forecast memory needs for new submissions~\cite{yoo2003slurm}.
These approaches demonstrate that, given sufficient historical examples, one can make informed predictions of memory consumption for recurring workloads.
In the realm of high-level numerical computing (e.g., NumPy-based scientific workflows), some have explored analytical modeling of the memory cost of array operations~\cite{cornell_memory_workshop}.
By understanding how operations such as matrix multiplication or convolution allocate output arrays and buffers, it is possible to estimate peak memory usage for a sequence of operations without fully executing them.
Likewise, in deep learning, several efforts aim to anticipate the memory footprint of neural network models~\cite{gao2020}.
Analytical frameworks exist to calculate the memory requirements of neural networks layer-by-layer based on input tensor shapes and network architecture~\cite{dell_3dunet_memory}.
Profiling tools and simulators also project a model’s memory usage before execution to assist with deployment decisions~\cite{tanash2019}.
These related works provide valuable insights and help guide the methodology of the present work.

The goal of the present chapter is to predict memory usage for new jobs given their input shapes, rather than to obtain an exact byte-by-byte usage.
In other words, the objective is to estimate the peak memory demand to a useful degree of accuracy—sufficient to guide resource allocation decisions.
Predicting memory usage implies focusing on whether a given amount of memory will be adequate for a workload, acknowledging that minor overheads and system-level factors can cause slight deviations in actual usage.
The approach emphasizes practical accuracy over perfect precision: an effective predictor might err by a small margin but should consistently avoid severe underestimation that leads to \ac{OOM} failures.
By targeting a reliable upper-bound estimate, the model can ensure robust memory reservations while still significantly narrowing the gap between requested and utilized memory compared to current heuristic practices.


\section{Materials and Methods}
\label{sec:pmc-materials-and-methods}

This section describes the end-to-end experimental methodology for gathering memory consumption data, generating features tied to input shapes, and training predictive models.
The pipeline consists of four primary phases:
\emph{data generation}, \emph{memory profiling}, \emph{result collection}, and \emph{analysis}.
The described workflow ensures consistency, reproducibility, and high-quality measurements suitable for training machine learning models that predict memory usage from shape attributes.

\vspace{1em}
\noindent
\textbf{Phase 1: Data Generation.}
The experiment begins by producing synthetic seismic datasets through the \texttt{generate\_data.py} script.
In a typical seismic context, three dimensions characterize the data:
\emph{inlines}, \emph{xlines}, and \emph{samples}.
The script systematically enumerates shape configurations by varying these three dimensions from an \texttt{INITIAL\_SIZE} to a \texttt{FINAL\_SIZE} with increments of \texttt{STEP\_SIZE}.
For instance, when these values are set to $100$, $600$, and $100$, respectively, the script generates volumes of sizes
$\{$ $(100,100,100),$ $(100,100,200),$ $\dots,$ $(600,600,600)$ $\}$
and writes each as a SEG-Y~\cite{barry1975segy} file (\texttt{.segy}).
These datasets serve as reproducible stand-ins for genuine seismic data, covering a wide range of volumes and highlighting how memory grows with dimension size.

\vspace{1em}
\noindent
\textbf{Phase 2: Memory Profiling.}
The \texttt{experiment.sh} shell script coordinates container-based jobs to run different seismic operators on each generated dataset.
Under \ac{HPC}-like constraints, jobs must be executed in an isolated manner to avoid interference (this is described in more detail on chapter~\ref{ch:measuring-memory-consumption}).
Accordingly, \texttt{experiment.sh} launches Docker-in-Docker (\texttt{dind}) containers, each pinned to a specific \ac{CPU} core through \texttt{--cpuset-cpus}, ensuring that runs are comparable.
The script then proceeds with three main operators:

\begin{itemize}
    \item \emph{Envelope~\cite{taner1979complex}}: A classic seismic attribute that calculates instantaneous amplitude for each trace.
    \item \emph{\ac{GST3D}~\cite{bigun2004recognition}}: A structural attribute that highlights seismic faults or discontinuities.
    \item \emph{3D Gaussian Filter~\cite{gonzalez2002digital}}: A spatial smoothing operator applied to seismic volumes.
\end{itemize}

Each operator is compiled into a Docker image (as defined in the provided \texttt{Dockerfile}), and each container execution uses the \texttt{collect\_memory\_profile.py} script to load and process the input data.
Internally, memory tracking is done through the \texttt{traceq} library, which measures \ac{RSS} over time as described in chapter~\ref{ch:measuring-memory-consumption}.
At completion, a \texttt{.prof} file is written, containing a time series of memory usage and metadata (e.g., timestamps, shape parameters).
Such profiling ensures peak memory usage is captured accurately.

\vspace{1em}
\noindent
\textbf{Executing Operators and Gathering Profiles.}
Listing~\ref{lst:exp_sh} provides an excerpt from \texttt{experiment.sh}.
The script organizes the flow:
(1) create Docker volumes,
(2) run \texttt{generate\_data.py},
(3) for each SEG-Y file, execute Envelope, \ac{GST3D}, or Gaussian Filter,
(4) run \texttt{collect\_results.py} to parse, clean, and consolidate the measurements, and
(5) finalize by invoking \texttt{analyze\_results.py} for exploratory data analysis.
Each run places output CSV files and profiles in designated paths under \texttt{OUTPUT\_DIR}.

\begin{lstlisting}[style=bashstyle,caption={Excerpts from \texttt{experiment.sh} orchestrating Docker-based runs. Variables like \texttt{DATASET\_FINAL\_SIZE} and \texttt{DATASET\_STEP\_SIZE} set shape range for dataset generation.}, label={lst:exp_sh}]
#!/usr/bin/env sh
TIMESTAMP="$(date +%Y%m%d%H%M%S)"
CPUSET_CPUS="0"
DATASET_FINAL_SIZE="800"
DATASET_STEP_SIZE="50"

echo "Generating input data..."
docker run \
  --cpuset-cpus=0 \
  -v "${DIND_VOLUME_NAME}:/var/lib/docker:rw" \
  ...
  --env EXPERIMENT_COMMAND="generate_data.py" \
  --env EXPERIMENT_ENV="\
    -e OUTPUT_DIR=/experiment/out \
    -e FINAL_SIZE=${DATASET_FINAL_SIZE} \
    -e STEP_SIZE=${DATASET_STEP_SIZE} \
  " \
  docker:28.0.1-dind \
  "/workspace/experiment.sh"

echo "Collecting memory profile for Envelope..."
for file in "${OUTPUT_DIR}/inputs"/*.segy; do
  filename=$(basename "$file" .segy)
  docker run \
    --cpuset-cpus=0 \
    ...
    --env EXPERIMENT_COMMAND="collect_memory_profile.py" \
    --env EXPERIMENT_ENV="\
      -e ALGORITHM=envelope \
      -e INPUT_PATH=/experiment/out/inputs/${filename}.segy \
    " \
    docker:28.0.1-dind \
    "/workspace/experiment.sh"
done
...
\end{lstlisting}

\noindent
Table~\ref{tab:env_vars} summarizes key environment variables used throughout these scripts.
They govern dataset sizes, repetition counts, and output paths, making the entire pipeline highly configurable.

\begin{table}[htbp]
    \centering
    \caption{Key environment variables for the experimental scripts.
    Defaults are shown in parentheses.
    \vspace{1em}}
    \label{tab:env_vars}
    \begin{tabular}{ll}
        \hline
        \textbf{Variable}             & \textbf{Description}                                                                   \\
        \hline
        \texttt{DATASET\_FINAL\_SIZE} & Final dimension size in \texttt{generate\_data.py} (e.g., 800)                         \\
        \texttt{DATASET\_STEP\_SIZE}  & Step size between dimension increments (e.g., 50)                                      \\
        \texttt{EXPERIMENT\_N\_RUNS}  & Number of repeated runs per shape (e.g., 30)                                           \\
        \texttt{CPUSET\_CPUS}         & CPU core pinning for Docker containers (e.g., 0)                                       \\
        \texttt{OUTPUT\_DIR}          & Directory for output profiles, logs, CSV files                                         \\
        \texttt{ALGORITHM}            & Selected operator (e.g., \texttt{envelope}, \texttt{gst3d}, \texttt{gaussian\_filter}) \\
        \hline
    \end{tabular}
\end{table}

\vspace{1em}
\noindent
\textbf{Phase 3: Results Collection and Feature Construction.}
Once the profiling completes, \texttt{collect\_results.py} parses every \texttt{.prof} file produced by \texttt{traceq}, extracts peak memory usage, and correlates it with the input shape.
Specifically, this script (1) reads inlines, xlines, and samples from the filename, (2) identifies the operator, (3) takes time-series memory usage and isolates the maximum, and (4) builds a consolidated dataframe.
During this step, it also derives higher-level features such as
\emph{total volume} ($\text{inlines} \times \text{xlines} \times \text{samples}$),
\emph{logarithmic transforms} (e.g., $\log_2(\text{volume})$),
\emph{surface area},
\emph{ratios of dimensions},
and \emph{polynomial expansions}.
These features capture different possible relationships between shape and peak memory usage.
Listing~\ref{lst:results_py} shows an excerpt of how \texttt{collect\_results.py} enumerates and transforms features.

\begin{lstlisting}[style=pythonstyle,
    caption={Feature extraction excerpt from \texttt{collect\_results.py}. Additional derived metrics help capture polynomial or ratio-based growth.},
    label={lst:results_py}]
df_features["volume"] = (
    df_features["inlines"]
    * df_features["xlines"]
    * df_features["samples"]
)

df_features["diagonal_length"] = np.sqrt(
    df_features["inlines"]**2
    + df_features["xlines"]**2
    + df_features["samples"]**2
)

df_features["surface_area"] = 2 * (
    df_features["inlines"] * df_features["xlines"]
    + df_features["inlines"] * df_features["samples"]
    + df_features["xlines"] * df_features["samples"]
)

df_features["log_inlines"] = np.log2(df_features["inlines"])
df_features["log_volume"] = np.log2(df_features["volume"])
...
\end{lstlisting}

\vspace{1em}
\noindent
\textbf{Phase 4: Model Training and Analysis.}
The same script (\texttt{collect\_results.py}) then trains multiple regression models, each leveraging \texttt{inlines}, \texttt{xlines}, \texttt{samples}, and derived features as predictors, with the target variable set to \emph{peak memory usage}.
A consistent training/test split (e.g., 80/20) ensures fair comparisons.
For each operator (envelope, \ac{GST3D}, Gaussian filter), performance metrics such as \textit{\ac{RMSE}}~\cite{hyndman2006}, \textit{\ac{MAE}}~\cite{willmott2005mae}, \textit{R\textsuperscript{2}}~\cite{draper1998applied}, and a custom \emph{accuracy} metric (fraction of predictions within a given relative error) are computed.
Additionally, an Optuna-based~\cite{akiba2019optuna} parameter search can adjust the weighting of these metrics to identify the most robust model under \ac{HPC} constraints, where underestimation can be especially harmful.
Below, we outline the key regression models considered:

\begin{itemize}
    \item \textbf{Linear Regression, Polynomial Regression}~\cite{hastie2009elements}\\
    \emph{Why use them?} These models provide simple baselines for capturing relationships between features and memory usage. Linear regression fits a straight line (or hyperplane) to the data, often revealing if memory usage scales roughly proportionally to the number of traces or samples. Polynomial regression extends this by incorporating higher-order terms to capture mild nonlinearities.\\
    \emph{Watch out for:} Purely linear models underfit when the real relationship is more complex, and high-degree polynomials risk overfitting.

    \item \textbf{Decision Trees}~\cite{breiman1984classification}, \textbf{Random Forest}~\cite{breiman2001random}, \textbf{Gradient Boosting (XGBoost)}~\cite{chen2016xgboost}\\
    \emph{Why use them?} These tree-based models can capture complex interactions and nonlinearities automatically.
    \begin{itemize}
        \item \emph{Decision Trees:} Simple to interpret and can highlight how certain threshold splits in \texttt{inlines}, \texttt{xlines}, or \texttt{samples} affect memory requirements.
        \item \emph{Random Forest:} An ensemble of trees trained on random data subsets; reduces variance and often yields better generalization.
        \item \emph{Gradient Boosting (XGBoost):} Sequentially builds an ensemble of weak learners (trees), typically achieving high accuracy in many structured-data problems.
    \end{itemize}
    \emph{Watch out for:} Overfitting if the trees grow too deep or the learning rate is set improperly. Hyperparameter tuning (e.g., via Optuna) is crucial.

    \item \textbf{Neural Networks (Feed-forward MLP)}~\cite{rumelhart1986learning}\\
    \emph{Why use them?} MLPs can approximate complex, highly nonlinear functions if given sufficient capacity, potentially uncovering intricate memory-usage relationships.
    \emph{Watch out for:} They often require larger datasets, careful hyperparameter tuning, and can be less interpretable. Overfitting is also a risk if the network is too large relative to the available data.

    \item \textbf{Gaussian Processes}~\cite{rasmussen2006gaussian}\\
    \emph{Why use them?} Provide both predictions and estimates of uncertainty, which is particularly helpful in \ac{HPC} contexts where underestimating memory can lead to system crashes.
    \emph{Watch out for:} They can be computationally expensive for large datasets (\(O(n^3)\) scaling) and require appropriate kernel choices.

    \item \textbf{Bayesian Ridge Regression}~\cite{bishop2006pattern}\\
    \emph{Why use it?} Extends basic linear models with Bayesian inference to estimate coefficients, yielding robust predictions and uncertainty measures—especially valuable for HPC tasks demanding safe memory estimates.
    \emph{Watch out for:} Like standard linear models, it may not capture highly nonlinear relationships without adding polynomial or interaction features.
\end{itemize}

Finally, \texttt{analyze\_results.py} visualizes memory usage distributions, regression residuals, feature selection tests, and data-reduction experiments.
This step provides deeper insight into how well each model generalizes, whether certain features are superfluous, and how many training points are truly required.
The user can, for instance, examine partial dependence plots or interpret tree-based models to see which dimensions exert the greatest effect on memory usage.

\vspace{1em}
\noindent
\textbf{Technical Considerations.}
\begin{itemize}
    \item \emph{Containerization and Reproducibility:}
    Every stage is executed in Docker containers that install the necessary dependencies (see the provided \texttt{Dockerfile} and references to Python tooling for \texttt{traceq}).
    Container images are built with pinned versions of \texttt{python}, \texttt{pip}, and other packages, ensuring results do not change due to environment drift.
    \item \emph{\ac{CPU} Affinity:}
    \texttt{--cpuset-cpus=0} pins processes to a single core, minimizing external noise.
    Repeated runs per dataset shape (\texttt{EXPERIMENT\_N\_RUNS}) add further robustness by averaging out minor system perturbations.
    \item \emph{Parallelization:}
    Though the Docker orchestration can scale to multiple \ac{CPU} cores, the design enforces isolated single-core experiments for consistency.
    In a real \ac{HPC} cluster, job schedulers would handle concurrency, but the underlying approach of shape-based memory profiling remains valid.
    \item \emph{SIGINT and Memory Cache Drops:}
    In \ac{HPC} environments, leftover memory from prior executions can skew usage measurements.
    The scripts optionally drop Linux memory caches to avoid carry-over effects, as recommended in prior memory-profiling guidelines explained on chapter~\ref{ch:measuring-memory-consumption}.
\end{itemize}

\vspace{1em}
\noindent
This unified pipeline, shown schematically in Figure~\ref{fig:pmc_datapipeline}, yields a high-fidelity dataset linking shape parameters to precise peak memory usage for multiple operators.
After collecting profiles and derived features, a diverse suite of regression models learns the mapping from $(\text{inlines}, \text{xlines}, \text{samples})$ to memory demand.
Subsequent analyses, including error breakdowns and comparisons across algorithms, clarify whether \emph{shape} alone is sufficient to accurately predict memory usage.
The following sections (Section~\ref{sec:pmc-results} and beyond) detail the results of these experiments and compare the performance of all selected models.

\begin{figure}[htbp]
    \centering
    % Placeholder for data pipeline figure
    \fbox{\rule{0pt}{1in}\rule{.9\linewidth}{0pt}}
    \caption{Schematic view of the pipeline: (1) generation of synthetic SEG-Y data, (2) container-based memory profiling per operator, (3) result aggregation with peak usage extraction and feature construction, and (4) regression model training and analysis.}
    \label{fig:pmc_datapipeline}
\end{figure}

%\section{Experimental Setup}
%\label{sec:pmc-experimental-setup}
%
%Software and Hardware Environment: Experiments were conducted in a Python-based HPC environment.
%The code is executed on [describe hardware – e.g., “a compute node with 256 GB RAM and 2× Intel Xeon CPUs”] or a similar platform, ensuring that even the largest test case can be run without swapping.
%We utilize Python libraries including NumPy for synthetic data generation, and TraceQ (a custom memory profiling tool) to measure memory consumption.
%TraceQ hooks into the process to log memory usage over time; from its output we extract the peak memory usage (maximum resident set size) during each run.
%This peak corresponds to the “Max Memory” metric reported by cluster job monitors (https://hpc.ncsu.edu/Documents/memory.php#:~:text=Max%20Memory%20%3A%20%20,08%20MB), which is the key quantity for resource planning.
%By capturing peak memory, we ensure our predictions aim at the worst-case usage during execution.
%
%Target Algorithm and Implementation: We focus on a representative tensor-based workload from seismic data processing.
%In particular, the code under test could be a function that takes a multi-dimensional seismic array (e.g., a 3D volume) and performs some compute-intensive operation (such as filtering, FFT-based processing, or attribute calculation).
%The specifics of the algorithm (e.g. complexity) are less important than its memory usage behavior.
%We treat it as a fixed “black box” whose memory footprint we want to predict.
%This function is invoked repeatedly with different input array shapes.
%All other factors (code path, data type, etc.) are held constant to isolate the effect of input shape on memory.
%
%Synthetic Dataset Generation: To train our models, we generated a synthetic dataset of input shapes and observed memory usages.
%We varied the input tensor dimensions systematically to cover a broad range of sizes.
%For example, for a 3D workload, we might vary depth, height, and width across realistic ranges (small, medium, large) and sample combinations of these.
%We ensured the synthetic shapes span from very small inputs (to observe baseline memory usage and any fixed overhead) up to near the upper limits of what the hardware could handle (to observe how memory scales at the high end).
%Each unique shape configuration was run through the target algorithm, and TraceQ recorded its peak memory.
%This process yields a labeled dataset: (features = shape metrics, target = memory).
%We repeated each measurement multiple times to verify consistency (minimal run-to-run variance in memory, which we indeed observed, indicating a deterministic memory pattern for given shapes).
%The synthetic approach allows gathering ample data (including extreme cases that might not appear in real datasets) under controlled conditions, which is ideal for training the more data-hungry models like neural nets.
%
%• Real-World Dataset (F3 Seismic) for Validation: In addition to synthetic data, we evaluated our models on a real seismic dataset – the Netherlands F3 block.
%The F3 dataset is a 3D seismic volume of size approximately 255 × 901 × 601 (depth × inline × crossline) (https://www.researchgate.net/figure/The-size-of-Netherlands-F3-dataset-is-255-901-601-depth-crossline-inline_fig3_373406803), totaling on the order of 138 million samples.
%This large volume is representative of high-end workload sizes in our domain.
%We ran the same target algorithm on the F3 data (and subsets or downsampled versions of it, if needed) to gather memory usage in practice.
%The F3 runs serve as an independent test: they allow us to see if models trained on synthetic patterns can generalize to a real-case scenario with a specific shape (and possibly slight differences, like the presence of geologic structures in the data, though that shouldn’t affect memory).
%In our experiments, the F3 full volume and several sub-volumes (for instance, splitting the volume into halves or quarters) were used as test points to evaluate prediction accuracy on real data shapes that were not explicitly in the training set.
%
%Memory Profiling Process: For each experiment (each input shape), we use TraceQ to record memory usage over the runtime of the algorithm.
%We extract the peak memory usage observed.
%This peak is our ground-truth label for the learning models.
%By focusing on peak, we inherently capture the worst-case memory requirement – exactly what needs to be reserved in an HPC job.
%We also note the timing of when the peak occurs (e.g., at a particular stage of the algorithm), though for modeling we only use the peak value.
%The overhead of TraceQ is negligible, and it provides high-resolution memory tracking, which gave us confidence in the accuracy of our measurements (within a few MB). Each shape configuration’s run is isolated (we ensured no other heavy processes on the node, memory was freed between runs, etc.).
%
%Training, Testing, and Validation Strategy: We split our collected data into a training set and a test set.
%The training set (mostly synthetic cases) is used to fit the models.
%We employed k-fold cross-validation on the training set to tune model hyperparameters (for example, to select the polynomial degree or tree depth that yields the best validation score).
%The held-out test set includes some synthetic shapes (to evaluate interpolation performance) and the real F3 cases as separate test points (to evaluate extrapolation or at least interpolation at a larger scale).
%Model performance is quantified using standard regression metrics like Mean Squared Error (MSE) and Coefficient of Determination (R²) on the test set.
%However, given our practical goal, we pay special attention to prediction bias: a model that consistently underestimates memory, even by a small percentage, is considered riskier than one that overestimates by a moderate amount.
%To capture this, we introduced a custom scoring metric that penalizes under-predictions more heavily than over-predictions.
%In practice, this could be a weighted error or a custom loss function where if predicted memory < actual memory, the penalty is amplified.
%We used this score to rank the models for “safety.” The metric reflects the fact that an underestimation could lead to a job crash (which is far worse than overestimation leading to some idle reserved memory) (https://pmc.ncbi.nlm.nih.gov/articles/PMC9906793/#:~:text=While%20these%20resource%20requirements%20are,waiting%20time%20for%20submitted%20jobs).
%Throughout training, we monitored this metric as well, and in model selection, we gave priority to models with little to no underestimation on validation data.
%
%Summary of Experimental Setup: In summary, our experiments involved hundreds of runs of the seismic algorithm with varying input shapes, automated memory tracking, and offline training of various predictive models.
%The combination of synthetic exhaustive sampling and real-case testing provides a robust assessment of whether input shape alone suffices to predict memory.
%Next, we present the results, including visual comparisons of predicted vs. actual memory usage and error analysis for each model type.
%
%
%\section{Experimental Results}
%\label{sec:pmc-results}
%
%Overall Prediction Accuracy: The results show that memory consumption can be predicted with high accuracy from input shapes. Most models captured the general trend that larger inputs require more memory, and several models achieved near-perfect fit on the test data. For instance, the best-performing model achieved an R² above 0.95 on the synthetic test cases, indicating that almost all variance in memory usage was explained by the input shape features. In concrete terms, for a typical large input in our test set (e.g., a 200×200×200 volume), the predicted peak memory differed from the actual by only a few percent. This level of accuracy is far better than the manual trial-and-error estimates users typically resort to, and is on par or better than similar memory prediction efforts reported in literature (which often report R² in the 0.7–0.9 range for more generalized workloads) (https://pmc.ncbi.nlm.nih.gov/articles/PMC9906793/#:~:text=Management%20%28Slurm%29,consumption%20of%20the%20HPC%20resources). Importantly, even when not exact, the predictions tended to err on the safe side, slightly overestimating rather than underestimating the memory in most cases – a desirable outcome for practical use.
%• Comparison of Models: We compared the performance of all models (Linear, Polynomial, Tree-based, Ensemble, Neural Network, Gaussian Process, Bayesian Ridge) using both the standard metrics and the custom underestimation-penalizing score. Key findings include:
%• Linear vs. Polynomial: The simple linear regression, using total number of elements as a feature, already provided a reasonable first approximation. It captured the general linear growth of memory with input size but under-predicted memory for very large inputs by a noticeable margin (e.g., for the largest synthetic case, linear prediction was ~10% lower than actual). This underestimation suggests a slight nonlinear component (perhaps a fixed overhead or higher-order term) that a pure linear model couldn’t capture. Polynomial regression (we experimented with quadratic and cubic terms) significantly improved the fit. A quadratic model, for example, reduced the error for large inputs and aligned closely with actual usage across the range, implying that the memory-size relationship has a curve that a second-order term can model. However, beyond a certain degree, polynomial models started to overfit the training data (oscillating between points), so we found a low-degree polynomial to be optimal.
%• Tree-Based Models: The single decision tree model did learn the relationship fairly well, creating splits mostly on the total number of elements feature. It had low error on training data but was slightly less accurate on test data due to some overfitting (the piecewise constant nature of a tree meant minor overshoots or undershoots in certain ranges). The power of tree models became evident with Random Forest and Gradient Boosting. The random forest smoothed out the decision tree’s piecewise predictions by averaging many trees, resulting in a robust model that was accurate to within a few percent for all test points. It rarely under-shot the actual memory – in fact, the ensemble’s tendency to average out noise meant it usually predicted a touch above the actual (providing a buffer). Gradient Boosting and XGBoost delivered the most accurate predictions among all models. After tuning (e.g., limiting tree depth to prevent overfitting small quirks), XGBoost achieved an almost one-to-one fit: when plotting predicted vs. actual memory, the points lay very close to the diagonal line. These ensemble models earned top scores in our custom metric as well, since they virtually never under-predicted the peak memory. This aligns with other research that found boosted trees to excel in resource usage prediction tasks (https://pmc.ncbi.nlm.nih.gov/articles/PMC9906793/#:~:text=from%20January%202019%20,consumption%20of%20the%20HPC%20resources).
%• Neural Network: Our neural network (a feed-forward model with a couple of hidden layers) also managed to learn the relationship after training. Its performance was comparable to the ensemble models in terms of mean error; however, one key difference was that the neural net exhibited slightly higher variance in error for different test cases. In other words, most predictions were spot-on, but a few were off by a larger margin (both over and under). We suspect this is due to the neural network not having as many training samples as it would ideally want – with more data or further hyperparameter tuning, it might match the consistency of the random forest. Nonetheless, it did not show any systematic bias and generally predicted safe values. The advantage of the neural network is that it could potentially incorporate additional inputs easily (if we had more features like algorithm settings), but in our shape-only scenario, it didn’t dramatically outperform the structured models.
%• Gaussian Process: The GP regressor yielded excellent results on the interpolation range of our synthetic data. Given its nature, it essentially provided an almost exact fit through the training points (since we assumed a relatively smooth kernel). On test points within the range of training shapes, the GP’s predictions were extremely close to actual usage, and it provided tight confidence intervals, signaling high certainty. One challenge with the GP emerged when predicting for the extreme ends (for example, the absolute largest shape which might be slightly beyond the range of training data). In those cases, the GP’s confidence interval widened, and we observed a slight underestimation for one extrapolation case. This is expected behavior – the model was less certain and effectively regressed toward the mean. In practice, this is useful: the GP can warn us when a prediction is an extrapolation by showing high uncertainty. For our purposes, the GP was among the top performers for interpolation, and careful handling of its predictions at extremes could make it a reliable choice.
%• Bayesian Ridge Regression: The Bayesian Ridge model performed similarly to the ordinary linear regression, as anticipated. The added regularization helped a bit in not overshooting any coefficients, but since the true relationship isn’t strictly linear, it also under-predicted for the largest inputs. It did, however, give a probabilistic prediction (with a variance) that indicated low confidence in those large-input predictions – which is an insight that plain linear regression wouldn’t provide. This mirrors the finding that a simple linear model is slightly inadequate, but the fact that it wasn’t wildly wrong means the assumption of memory roughly scaling with input size is fundamentally sound.
%• Quantitative Performance Summary: (We will present a table or chart here comparing each model on metrics like MSE, R², and the custom score, as well as noting under/over prediction tendencies.) In summary, the ensemble tree models (Random Forest, XGBoost) and the Gaussian Process were the stand-out performers with highest accuracy and safest predictions. For example, XGBoost achieved an R² ≈ 0.98 on the test set and had a custom score indicating virtually zero dangerous underestimations. The neural network was a close second-tier, with R² in the ~0.95 range. Polynomial regression (quadratic) also did quite well (R² ~0.92), proving that a fairly simple curve-fit already captures most of the memory behavior. Linear regression and Bayesian Ridge trailed with R² in the 0.85–0.90 range, mainly due to systematic underestimation on large cases. The single decision tree had high training R² but test R² around 0.88, reflecting some overfit piecewise behavior. These results confirm that the relationship between input shape and memory is learnable and mostly smooth, as simpler models weren’t far off, but the best accuracy is achieved with models that can handle a slight nonlinearity and interaction in the data.
%• Actual vs. Predicted Plots: We include plots of actual vs. predicted memory for selected models to visually illustrate performance. In the case of XGBoost (our best model), the scatter plot of predicted vs. actual memory (in GB) for all test points lies almost on the $y=x$ line, with perhaps a slight buffer above the line for the largest points (indicating slight overestimation). This visual confirms the high accuracy. For linear regression, the plot shows a trend line that falls under the $y=x$ line at the high end, consistent with under-predicting large values. We also plot the error (residual) vs. input size, which shows linear regression’s residuals growing with input size, whereas the residuals for XGBoost stay centered around zero across the whole range. These figures reinforce why the more sophisticated models were needed for top precision. (If space allows, we might also show the GP’s prediction with confidence bands as a function of input size – demonstrating how uncertainty grows outside the training range).
%• Performance on Real F3 Data: A crucial test was how well the models trained on synthetic data predict the memory usage for the real F3 seismic dataset. We found that the best models generalized well. For instance, the random forest and XGBoost predictions for the full F3 volume’s memory were within ~5% of the actual peak usage observed when we ran the algorithm on that data. This is a strong result, indicating that our synthetic training (which included shapes similar in scale to F3) was representative. The models essentially recognized that F3’s shape (255×901×601) was at the high end of the range and extrapolated the trend accurately. The neural network and GP also predicted close to the actual F3 memory, each slightly overestimating by a small margin (which in a real scenario is acceptable, as it would mean we’d allocate a bit more memory than needed, avoiding any risk). Simpler models like linear regression under-shot the F3 usage by more than 10% (which, if followed blindly, could have led to under-allocation). This validates our approach: by using more expressive models, we can trust the predictions even for a real-case input that was not explicitly in training. It also highlights the benefit of including high-end synthetic points in training so that models don’t need to extrapolate far beyond seen data.
%• Custom Metric & Safe Predictions: Using our custom “no-underestimation” score, we determined that several models (especially Random Forest, XGBoost, and GP) achieved perfect or near-perfect safety scores – meaning they never predicted less than the actual usage by more than a negligible amount on any test case. Models that had any significant misses (like linear regression’s 10% underestimation on the largest case) were ranked lower despite decent R². This exercise underscores an important point: what constitutes the “best” model depends on the practical criterion. If avoiding job crashes is paramount, one might choose a model that perhaps overestimates by a margin on every single point rather than one that is 99% accurate on average but occasionally 10% low. Fortunately, in our situation, we found models that were both highly accurate and safe. In deployments, one could even take an accurate model and add a small safety buffer to its predictions (e.g., predict memory = model output * 1.05 to cover any unknowns). In our results, we note that approach as a possible refinement, though for the top models it wasn’t really necessary given their performance.
%• Insights Gained: Through analysis of the learned models, we can glean insights about how the algorithm uses memory. The most dominant feature was unsurprisingly the total number of data points (or a close proxy like one dimension times another for 3D). This confirms the intuition that more data yields higher memory usage primarily linearly. The need for a quadratic term in polynomial regression or depth in trees suggests there is a fixed overhead or a second-order effect – possibly the algorithm allocates an extra array that scales with one of the dimensions (for example, maybe it allocates a temporary 2D slice array of size width×height, causing an additional term proportional to one plane of the volume). The models picked up on that. In fact, by inspecting the decision splits of the tree or the coefficients of a quadratic fit, we could guess at the formula: one such analysis hinted at $M \approx a \cdot (D \times H \times W) + b \cdot (H \times W) + c$ bytes (where the second term represents an extra array the size of one horizontal slice). This matches how one might manually derive the memory formula if one knew the algorithm’s internals. Thus, the ML approach not only provides predictions but can also validate our understanding of algorithmic memory usage.
%• Error Analysis: We also examined the few cases with larger prediction errors. One pattern observed was that when an input shape was very small, some models (especially polynomial and ensemble) slightly over-predicted memory. For example, for a tiny 10×10×10 test volume, the actual memory usage might include a base overhead (say 50 MB for loading libraries, etc.), which our models (trained mostly on larger sizes) approximated less precisely, leading to a prediction of perhaps 60 MB vs actual 55 MB. This is a minor issue, and in practice overallocation of a few MB is inconsequential, but it points to the models being primarily tuned for the scaling behavior rather than the absolute base offset. We could address this by including more small-size data points in training or by explicitly modeling base overhead as a constant term (which linear models do automatically as an intercept, but tree models might not capture as smoothly). We mention this for completeness, but emphasize it’s not a serious flaw.
%• Statistical Confidence: Finally, we assess the confidence in these results. Given the consistent performance across multiple validation splits and the agreement of different high-capacity models (trees, NN, GP) on the memory trend, we are confident that the dependency of memory on input shape has been learned robustly. The standard deviation of errors is low, and no model completely disagreed with theoretical expectations (no bizarre predictions). This consistency suggests the problem is well-conditioned for learning. The next section will discuss the implications of these findings and how such a predictive model can be applied in practical settings.
%
%
%\section{Conclusion}
%\label{sec:pmc-conclusion}
%
%•	Feasibility of Shape-Based Memory Prediction: Our investigation confirms that predicting memory consumption from input shapes is not only feasible but can be highly accurate for tensor-based workloads. Even using relatively straightforward features (dimensions, their products) and general-purpose ML models, we achieved predictions that are within a few percent of actual usage for both synthetic tests and real seismic data. This validates the core hypothesis of this chapter: for data-intensive computations where the algorithm doesn’t drastically change behavior based on data content, input size is the primary driver of memory demand. We have shown that a model can learn this relationship, essentially automating what an expert might deduce (e.g., “if I double the grid size, memory will roughly double”) with far greater precision. In practical terms, this means developers and HPC users can rely on a predictive system to estimate memory needs instead of guesswork.
%•	Benefits for Seismic Workflows: In the seismic domain, datasets are large and processing them is memory-intensive. By deploying a memory predictor trained as we described, geoscientists can plan their computational jobs more efficiently. For example, before launching a seismic inversion or attribute extraction on a new survey, one can input the survey’s dimensions into the predictor to get an estimate of peak memory required. This helps in choosing an appropriate computing node or cluster configuration (ensuring enough memory is available) and in reserving just the right amount of memory. The outcome is fewer failed jobs due to OOM (out-of-memory) errors and less wasted resources from over-allocation. It also enables what-if analyses: users can simulate how memory demand grows if, say, they increase the resolution of their grid or the size of a processing batch, guiding decisions about data resizing or algorithm parameter tuning. For the F3 dataset example, our model could have immediately informed the user that on a 256 GB node, the job will fit with room to spare, whereas on a 64 GB node it would likely exceed available memory – avoiding an attempt that would inevitably crash. Such foresight is extremely valuable when pipeline execution time is measured in hours or days; it prevents lost time due to memory errors.
%•	Cluster Resource Management Improvements: More broadly, integrating these predictive models into cluster scheduling systems can improve overall HPC operations. If each job can come with a more accurate memory requirement estimate (either provided by an offline tool or an automated script that uses input metadata to predict), the scheduler can pack jobs more optimally and reduce idle memory. This translates to higher throughput and utilization of expensive HPC resources (https://pmc.ncbi.nlm.nih.gov/articles/PMC9906793/#:~:text=the%20time,consumption%20of%20the%20HPC%20resources). As noted in related work, when users specify resources more accurately, it reduces queue wait times and increases system efficiency (https://pmc.ncbi.nlm.nih.gov/articles/PMC9906793/#:~:text=While%20these%20resource%20requirements%20are,waiting%20time%20for%20submitted%20jobs).
%Our approach could complement such systems as a plugin: for instance, a submission script could call a “memory predictor” given the input file dimensions before filling in the resource request. Some HPC centers are already exploring machine learning to assist users in resource selection (https://pmc.ncbi.nlm.nih.gov/articles/PMC9906793/#:~:text=,waiting%20time%20for%20submitted%20jobs),
%and a shape-based predictor can be one piece of that puzzle, especially for regular data-processing jobs. Additionally, from a power and cost perspective, avoiding overallocation has tangible benefits (power savings from not needlessly reserving memory and spinning up large nodes) (https://pmc.ncbi.nlm.nih.gov/articles/PMC9906793/#:~:text=the%20time,consumption%20of%20the%20HPC%20resources)
%
%	•	\textbf{Generality to Other Domains:} While our case study centered on a seismic processing algorithm, the methodology and benefits extend to other high-dimensional data processing tasks. Any application that primarily handles large arrays or matrices could use a similar predictive model. For example, in climate modeling or CFD simulations, one could predict memory from the grid resolution; in bioinformatics, from the genome length or number of sequences being processed; in deep learning, from the model architecture and input image size (in fact, researchers have built models to estimate memory usage of neural networks given layer configurations (https://infohub.delltechnologies.com/sv-se/l/memory-consumption-modeling-of-deep-learning-workloads/case-study-application-3d-u-net/#:~:text=intermediate%20tensors%20,depends%20on%20four%20key%20parameters)
%The common theme is that \textbf{a few key parameters define the problem size}, and thus memory. By training models on those parameters, one can obtain quick estimates without running the full application. This is akin to performance modeling efforts in HPC, but focused on memory rather than runtime. Our work demonstrates a blueprint for building such models with modest effort and data.
%
%
%	•	\textbf{Practical Deployment Considerations:} To implement this in practice, one would integrate the trained model into the workflow. The model itself (especially if using a simple regressor or tree ensemble) can be saved and embedded in a small CLI tool or library. Given an input shape (perhaps read from the data header or inferred from input), the tool outputs an estimated peak memory. Users might still include a safety margin (e.g., request 10% above the estimate) to account for any unforeseen overhead or to satisfy cluster integer memory units. Since our results indicated robust performance, that margin can be small. It’s worth noting that if the range of input sizes changes (say, even bigger data than seen before), the model should be retrained or at least updated with new samples – an ongoing learning process. However, gathering a few new sample points (by profiling the new extreme cases) and retraining is far less onerous than continuously guessing and checking. The cost of profiling is just a few runs of the program (which one might do in testing anyway), and after that the model spares you from surprises.
%
%
%	•	\textbf{Limitations and Future Work:} Our approach deliberately simplifies the problem by assuming a single application and focusing only on input shape. In more complex scenarios, memory usage could also depend on other factors: e.g., algorithmic parameters (tolerance settings that change iteration counts), data values (sparse vs dense data might use memory differently), or concurrent workloads on the same node. For our seismic use case, these were not significant factors, but in general, expanding the feature set might further improve predictions if needed. Future work could explore \textbf{multi-variable models}, incorporating additional inputs like algorithm flags or even real-time performance counters. Another avenue is to create \textbf{dynamic predictors} that predict not just the peak but the timeline of memory usage, enabling optimizations like when to start transferring data out to disk, etc. Moreover, applying this method to \textit{runtime} prediction alongside memory would give a full picture of resource needs – something HPC schedulers would highly value (https://www.hpcwire.com/solution_content/ibm/cross-industry/a-crystal-ball-for-hpc/#:~:text=A%20Crystal%20Ball%20for%20HPC,runtime%20required%20for%20their%20workloads)
%
%
%	\textbf{Conclusion:} In conclusion, this chapter demonstrated a successful case of using machine learning to predict peak memory consumption from input shapes for computational workloads. We transformed a traditionally heuristic, trial-and-error task into a data-driven modeling task, and achieved high accuracy and reliability. The predictive models effectively serve as a “crystal ball” for memory usage, allowing us to anticipate resource demands before execution. The implications are substantial: with such tools, scientists and engineers can run large-scale computations with greater confidence and efficiency. No longer must one “overestimate by a large margin just in case” (https://pmc.ncbi.nlm.nih.gov/articles/PMC9906793/#:~:text=While%20these%20resource%20requirements%20are,waiting%20time%20for%20submitted%20jobs)
%– instead, one can request resources smartly, guided by predictions. This fosters better usage of HPC infrastructure and smoother user experience. The success with the seismic data example paves the way for adopting similar strategies in other domains dealing with big data and arrays. Ultimately, predictive memory modeling contributes to the broader goal of \textbf{autonomous resource management} in computing – where systems can self-tune and allocate resources optimally based on learned models, minimizing human guesswork and error. The next chapter will build on these insights, possibly exploring runtime performance prediction or integrating our memory predictor into a live scheduling system, to further illustrate the power of predictive modeling in HPC environments.